---
title: "ESC HW2"
author: '2015122016 김주연'
date: "2019년 3월 25일"
output: html_document
---



#Q1
The Cautions for Regression Through Origin   

* The sum of residuals is not zero.     
$\sum_{1}^{n}e_{i}\neq 0$     
* THe weighted sum of residuals is zero.    
$\sum_{1}^{n}X_{i}e_{i} = 0$ & $\sum_{1}^{n}\widehat{Y_{i}}e_{i} = 0$   
* SSE may exceed SSTO of the full model, so 1-(SSE/SSR) can be negative.    


$$
\sum_{1}^{n}\left(y_{i}-\overline{y}\right)^{2} = \sum_{1}^{n}\left(y_{i}-\widehat{y_{i}}\right)^2 + \sum_{1}^{n}\left(\widehat{y_{i}}-\overline{y}\right)^{2} + 2\sum_{1}^{n}\left(y_{i}-\widehat{y_{i}}\right)\left(\widehat{y_{i}}-\overline{y}\right)
$$

일반적인 constant를 포함한 linear regression model에서는 위의 공식에서 $\sum_{1}^{n}\left(y_{i}-\widehat{y_{i}}\right)\left(\widehat{y_{i}}-\overline{y}\right)$항이 0이 되기 때문에      
SSTO = SSE + SSR을 이용하여 $R^{2}$값을 구할 수 있다.     
하지만 constant가 없는 regression through origin model에서는 
$\sum_{1}^{n}\left(y_{i}-\widehat{y_{i}}\right)\left(\widehat{y_{i}}-\overline{y}\right)$항이 0이 되지 않아     
$SSTO \neq SSE + SSR$이 되고, 일반적인 $R^{2}$ 공식을 사용할 수 없게 된다.     
특히 평균을 예측하는 것만으로도 오차가 충분히 작을 때 regression through origin model은 poor fitting을 하게 되고,       
$\sum_{1}^{n}\left(y_{i}-\overline{y}\right)^{2}$보다     $\sum_{1}^{n}\left(\widehat{y_{i}}-\overline{y}\right)^{2}$이 더 큰 값을 갖게 되어 $R^{2}$가 음수의 값을 갖게 된다.    



#Q2
```{r setup, include=F}
library(tidyverse)
library(corrplot)
library(car)
library(MASS)
library(leaps)
library(glmnet)
```

##Data Structure
```{r}
boston <- read.csv("file:///C:/Users/Lucy Jooyeon Kim/Desktop/ESC/2019-1/[Week 2] Assignment/dataset/boston.csv")

glimpse(boston)
summary(boston)
```

##Check normality of response variable
```{r}
hist(boston$medv)
boston %>% ggplot(aes(medv)) + stat_density() +  theme_bw()
```

약간 skewed된 것 같긴 하지만 roughly normal distribution을 따른다고 볼 수 있을 것 같다.

#EDA
```{r}
colSums(is.na(boston))

par(mfrow=c(2,2))
plot(boston$crim, boston$medv)
plot(boston$zn, boston$medv)
plot(boston$indus, boston$medv)
plot(boston$nox, boston$medv)
plot(boston$rm, boston$medv)
plot(boston$age, boston$medv)
plot(boston$dis, boston$medv)
plot(boston$rad, boston$medv)
plot(boston$tax, boston$medv)
plot(boston$ptratio, boston$medv)
plot(boston$black, boston$medv)
plot(boston$lstat, boston$medv)
```

```{r}
par(mfrow=c(1,1))
corrplot(cor(boston))

cor(boston$rad, boston$tax)
cor(boston$nox, boston$dis)
cor(boston$nox, boston$indus)
cor(boston$age, boston$dis)
cor(boston$nox, boston$age)
cor(boston$tax, boston$indus)
cor(boston$indus, boston$dis)
```

correlation coefficients가 0.7 이상인 설명변수들로 이들 사이에 다중공선성이 발생할 가능성이 높다고 판단.    
특히 rad와 tax 사이에 상당히 높은 선형적 관계가 있을 것으로 추정.    

```{r}
plot(boston$rad, boston$tax)

par(mfrow=c(1,2))
hist(boston$rad)
hist(boston$tax)
```

rad와 tax 사이의 scatter plot을 그려보니 outlier에 의해 correlation이 발생하는 것으로 보여 histogram을 그려본 결과     
peak가 두 개인 data임을 확인할 수 있고, 이에 대한 처리가 필요할 것으로 보임.   

```{r}
par(mfrow=c(2,2))
plot(boston$nox, boston$dis)
plot(boston$dis, boston$age)
plot(boston$nox, boston$age)
plot(boston$nox, boston$indus)
plot(boston$tax, boston$indus)
plot(boston$dis, boston$indus)
```

nox, dis, tax, age, indus 사이에 어느 정도의 correlation이 있는 것으로 보이며, 
age와 indus는 nox, tax, dis 등으로 어느 정도 설명이 될 수 있을 것으로 판단됨.   
(역의 관계도 성립할 가능성 존재)

#full model
```{r}
full <- lm(medv ~ . , data=boston)
summary(full)
```

우선 모든 설명변수들을 넣고 full model을 만들어본 결과 indus와 age의 계수가 유의하지 않게 나왔음.

```{r}
plot(full)
vif(full)
AIC(full) ; BIC(full)
```

#expected regression model
```{r}
boston2 <- boston[-c(369,372,373),]
exp.md <- lm(medv ~ .-indus-age, data=boston2)
summary(exp.md)
plot(exp.md)
vif(exp.md)
AIC(exp.md) ; BIC(exp.md)
```

outlier로 예상되는 3개의 점을 제외하고, 유의하지 않았던 설명변수들을 빼서 regression model을 세워보았음.   
이 regression model과 variable selection으로 얻는 regression model을 비교해보고자 함.   

위에서 언급했던 rad와 tax의 collinearity를 해결해보기 위해    
두 가지의 가능성을 시도해보았지만 적합하지 않다고 판단,    
outlier 3개를 제외한 data로 variable selection을 진행하기로 함.   

##case1 - make them binary : worse
```{r}
boston_re <- boston
boston_re$taxhigh <- ifelse(boston_re$tax > 600, 1, 0)
boston_re$radhigh <- ifelse(boston_re$rad > 20, 1, 0)
summary(boston_re)

full.try1 <- lm(medv ~ .-indus-age-tax-rad, data=boston_re)
summary(full.try1)
vif(full.try1)
```

##case2 - remove high values : BIAS
```{r}
boston_nohigh <- boston[-c(357:493),]
dim(boston_nohigh)[1]
hist(boston_nohigh$rad)
hist(boston_nohigh$tax)
corrplot(cor(boston_nohigh))

full.try2 <- lm(medv ~ .-indus-age, data=boston_nohigh)
summary(full.try2)
vif(full.try2)
AIC(full.try2) ; BIC(full.try2)

dim(boston[boston$tax > 600,])[1]
dim(boston[boston$rad >20,])[1]
dim(boston)[1]
```

##variable selection by leaps package
```{r}
regfit.fwd <- regsubsets(medv~.,boston2,nvmax=13,method='forward')
sum.fwd <- summary(regfit.fwd)
which.max(sum.fwd$adjr2)
which.min(sum.fwd$bic)
which.min(sum.fwd$cp)
sum.fwd

red.fwd <- lm(medv ~ .-indus, data=boston2)
summary(red.fwd)

red.fwd <- lm(medv ~ .-age-indus, data=boston2)
summary(red.fwd)
plot(red.fwd)
vif(red.fwd)
AIC(red.fwd) ; BIC(red.fwd)
```

```{r}
regfit.bk <- regsubsets(medv~.,boston2,nvmax=13,method='backward')
sum.bk <- summary(regfit.bk)
which.max(sum.bk$adjr2)
which.min(sum.bk$bic)
which.min(sum.bk$cp)
sum.bk

red.bk <- lm(medv ~ .-indus-age, data=boston2)
summary(red.bk)
plot(red.bk)
vif(red.bk)
AIC(red.bk) ; BIC(red.bk)
```

```{r}
regfit.sw <- regsubsets(medv~.,boston2,nvmax=13,method='seqrep')
sum.sw <- summary(regfit.sw)
which.max(sum.sw$adjr2)
which.min(sum.sw$bic)
which.min(sum.sw$cp)
sum.sw

red.sw2 <- lm(medv ~ .-indus-age-chas, data=boston2)
summary(red.sw2)  
plot(red.sw2)
vif(red.sw2)
AIC(red.sw2) ; BIC(red.sw2)
```

#variable selection by MASS package
```{r}
null <- lm(medv ~ 1, data=boston2)

fwd.model <- stepAIC(null, direction = "forward", trace = FALSE,
                     scope=list(upper=full,lower=null))
summary(fwd.model)
plot(fwd.model)
vif(fwd.model)
AIC(fwd.model) ; BIC(fwd.model)

full <- lm(medv ~ ., data=boston2)

bk.model <- stepAIC(full, direction = "backward", trace = FALSE)
summary(bk.model)
plot(bk.model)
vif(bk.model)
AIC(bk.model) ; BIC(bk.model)

step.model.f <- stepAIC(full, direction = "both", trace = FALSE)
summary(step.model.f)
step.model.n <- stepAIC(null, direction = "both", trace = FALSE,
                       scope=list(upper=full,lower=null))
summary(step.model.n)

step.model <- step.model.f
plot(step.model)
vif(step.model)
AIC(step.model) ; BIC(step.model)
```

###summary
method | # of vars | adj $R^{2}$ | AIC | BIC
-|-|-|-|-
full | 13(11) | 0.7338 | 3027.609 | 3091.007
expected | 11(11) | 0.7722 | 2904.767 | 2959.635
forward | 11(11) | 0.7722 | 2904.767 | 2959.635
backward | 11(11) | 0.7722 | 2904.767 | 2959.635
fwd/bk(adj.R) | 12(11) | 0.7723 | 2905.598 | 2964.687
stepwise | 11(11) | 0.7722 | 2904.767 | 2959.635
stepwise(BIC) | 10(10) | 0.7693 | 2910.358 | 2961.005


Thus, the final selected regression model is
$$\widehat{Y_{i}}= 30.3139 -0.0991 * crim + 0.0399 * zn + 2.1398 * chas - 16.1581 * nox + 4.4888 * rm - 1.2949 * dis + 0.2511 * rad - 0.0122 * tax - 0.9542 * ptratio + 0.0087 * black - 0.4272 * lstat$$

#cross-validation
```{r}
y = boston$medv
x = model.matrix(medv~., boston)[,-1]
x = scale(x) 
grid = 10^seq(10, -2, length=100)

set.seed(1234)
train = sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
```

train data와 test data를 만들기 위해 train inddex와 test index를 random하게 선정.

#ridge regression
```{r}
rdg.cv.out = cv.glmnet(x[train,], y[train], alpha=0)
plot(rdg.cv.out)
rdg.best.lambda = rdg.cv.out$lambda.min
rdg.best.lambda

rdg.mod <- glmnet(x[train,], y[train], alpha=0, lambda= grid)
rdg.pred <- predict(rdg.mod, s=rdg.best.lambda, newx=x[test,])

mean((rdg.pred - y.test)^2)
mean((mean(y[train])-y.test)^2)
```

최적의 lambda 값으로 fitting한 ridge regression model의 test MSE와 null model의 test MSE 비교

#lasso regression
```{r}
las.cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(las.cv.out)
las.best.lambda = las.cv.out$lambda.min
las.best.lambda

las.mod <- glmnet(x[train,], y[train], alpha=1, lambda= grid)
las.pred <- predict(las.mod, s=las.best.lambda, newx=x[test,])

mean((las.pred - y.test)^2)
mean((mean(y[train])-y.test)^2)
```

최적의 lambda 값으로 fitting한 lasso regression model의 test MSE와 null model의 test MSE 비교